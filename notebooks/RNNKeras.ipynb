{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "### Vanilla neural networks and  convolutional neural networks also accept a fixed size vector input and porudce a fixed size vector output\n",
    "### These models perform this mapping using a fixed amount of computational steps\n",
    "### RNNs on the other hand allow us to operate over sequences of  vectors - in the inputs, outputs or most generally both\n",
    "<img src = 'recnn.jpg'/>\n",
    "<!-- img src = 'house_generate.gif'/-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent nets are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  have an input  consisting of a sequence of entities  $x^{(1)}\\;x^{(2)} \\dots \\; x^{(r)}\\;$ \n",
    "### Corresponding to this input we need to either produce a sequence $y^{(1)}\\;y^{(2)} \\dots y^{(r)}$ or just one output for the entire input sequence y\n",
    "### We denote the output the RNN produces by $\\hat{y}^{(1)}, \\hat{y}^{(2)} \\dots \\hat{y}^{(r)}$\n",
    "### The case where RNN produces one output for every entity in the single input can be descibed using the following equations:\n",
    "<h3><center>$h^{(t)} = tanh(Ux^{(t)} + Wh^{(t-1)} + b)$</center></h3>\n",
    "<h3><center> $\\hat{y}^{(t)}=softmax(Vh^{(t)}+c)$</center></h3>\n",
    "### The RNN computation involves first computing the hideen state for an entity in the sequence denoted by $h^{(t)}$ using the corresponding input at $x^{(t)}$ and the previous hidden state at $h^{(t-1)}$ with weights associated with these denoted by U and W and a bias term b\n",
    "### There are weights associated with the hidden state denoted by V and a bias term c\n",
    "### The parameters of the RNN, namely, U,W,V,b,c, etc. are shared across the computation of the hidden layer and output value (for each of the entities in the sequence)\n",
    "<img src='rnn_eqn_diagram.jpg'/>\n",
    "## RNN variant with recurrence carried out using output produced in the previous step rather than the previous hidden state\n",
    "<img src='rnn_prev_output.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs\n",
    "### The key intuition behind a bidirectional RNN is to use the entities that lie further in the sequence to make a prediction for the current entity\n",
    "### It can be described using the following equations:\n",
    "<h3><center>$h_f^{(t)}=tanh(U_fx^{(t)} + W_f^{(t)} + b_f)$</center></h3>\n",
    "<h3><center>$h_b^{(t)}=tanh(U_bx^{(t)} + W_b^{(t)} + b_b)$</center></h3>\n",
    "<h3><center>$\\hat{y}^{(t)}=softmax((V_bh_b^{(t)} + V_fh_f^{(t)}+c)$</center></h3>\n",
    "### The parameters of the RNN, namely, $U_f$ , $U_b$, $W_f$ , $W_b$, $V_b$, $V_f , b_f , b_b$, c, etc. are shared across the computation of the hidden layer and output value (for each of the entities in the sequence). The RNN as described by the equations can process an arbitrarily large input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients vanishing, explosion and clipping\n",
    "### During RNN training gradients can drop to a very low value - vanish, shoot up to very large values - exploding\n",
    "### The technique used is gradient clipping which rescales the norms of the gradients if it shoots up above a user defined threshold which introduces an extra hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory Networks\n",
    "### The god of batting is Sachin. If we have to from this sentence predict the last word for the phrase The god of batting is .... - it is fairly straight forward\n",
    "### Lets consider this set of utterances. I watched Sholay as a kid. It was a fabulous action movie. I took to action movies. Then i went to watch Golmaal. It was an amazing comedy. I feel like watching an old comedy. I will check out again .....   . Here to plug in Golmall we need to go back into the statement and figure it out. We need to remember a hell of a lot more to do this\n",
    "### LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "### LSTMs work on the basis of cell states, input gates and forget gates.  For example upon encountering the wish to see an old comedy it can forget information on action movies\n",
    "### An LSTM can be described with the following set of equations:\n",
    "<h3><center>$z^{(t)}=g(W_zx^{(t)} + R_z \\hat{y}^{(t-1)} + b_z)$</center></h3>\n",
    "<h3><center>$i^{(t)}=\\sigma(W_ix^{(t)} + R_i \\hat{y}^{(t-1)} + p_i \\otimes c^{(t-1)} + b_i)$</center></h3>\n",
    "<h3><center>$f^{(t)}=\\sigma(W_fx^{(t)} + R_f \\hat{y}^{(t-1)} + p_f \\otimes c^{(t-1)} + b_f)$</center></h3>\n",
    "<h3><center>$c^{(t)}=i^{(t)} \\otimes z^{(t)} + f^{(t)} \\otimes c^{(t-1)}$</center></h3>\n",
    "<h3><center>$o^{(t)}=\\sigma(W_ox^{(t)} + R_o \\hat{y}^{(t-1)} + p_o \\otimes c^{(t)} + b_o)$</center></h3>\n",
    "<h3><center>$\\hat{y}^{(t)}=o^{(t)}\\otimes h(c^{(t)})$</center></h3>\n",
    "### The most important part is the cell state denoted by $c^{(t)}$\n",
    "### The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. Itâ€™s very easy for information to just flow along it unchanged\n",
    "### It is updated based on the block input $z^{(t)}$ and the previous cell state $c^{(t-1)}$. The input gate $i^{(t)}$ determines how much of the block makes it into the input the forget gate $f^{(t)}$ how much to the previous cell state to retain\n",
    "### All the p terms are peephole connections which allow for a fraction of the cell state to factor in the computation of the term in question\n",
    "<img src='LSTM.JPG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 32124\n",
      "total chars: 50\n",
      "nb sequences: 10695\n",
      "\n",
      "Sentences first few elements are ['preface\\n\\n\\nsupposing that truth is a woma', 'face\\n\\n\\nsupposing that truth is a woman--', 'e\\n\\n\\nsupposing that truth is a woman--wha', '\\nsupposing that truth is a woman--what t', 'pposing that truth is a woman--what then']\n",
      "\n",
      "next_chars first 100 elements are  ['n', 'w', 't', 'h', '?', 's', 'h', 'e', 'o', 'g', 'u', '\\n', 'r', 'u', 'e', 'i', ' ', 'a', 'a', ' ', 'i', 's', 'h', 's', 'i', 's', 'f', ' ', ' ', 'e', 'h', 'e', 'e', '\\n', 'g', 't', 't', ' ', 'v', 'f', 'l', ' ', ' ', 'd', 's', 'n', 'w', 'e', '-', 'a', 't', ' ', 'r', 'b', '\\n', 'r', 'u', 'e', ' ', 'd', 'l', 's', 'i', 'o', 'u', 't', 'w', 'h', 'h', 'h', 'h', ' ', 'v', 'u', 'a', 'y', 'a', '\\n', 'e', ' ', 'd', 's', 's', 'o', 'r', 'h', 'h', 'e', 'e', ' ', 's', 'l', 'd', 'n', 'u', 'e', 'l', 'm', 'h', 's']\n",
      "Vectorization...\n",
      "x and y shapes after vectorization are (10695, 40, 50), (10695, 50) respectively\n",
      "x and y first rows after we have populated vectors with 1s for characters respectively are [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False], [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open('nietzsche500.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('\\nSentences first few elements are {}'.format(sentences[:5]))\n",
    "print('\\nnext_chars first 100 elements are ', next_chars[:100])\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "print('x and y shapes after vectorization are {}, {} respectively'.format(x.shape, y.shape))\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('x and y first rows after we have populated vectors with 1s for characters respectively are {}, {}'.\n",
    "     format(x[0, 0, :], y[0, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "print(len)\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# model dimensions here will be 4 * ((50 + 1) * 128 + 128 * 128)\n",
    "# 50 is the size of the input layer, 128 is the size of the output layer\n",
    "# U will be of dimension (50 + 1) * 128 , W will be of dimension 128 * 128 \n",
    "# there will be a different set of matrices for the input, forget and output gate\n",
    "# We will have one more matrix for the cell state \n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "10695/10695 [==============================] - 14s 1ms/step - loss: 0.5583\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t europe, furnishes food for thought and\"\n",
      "t europe, furnishes food for thought and to do des ane ventains of which as mose of rus is be the meally to peryable by weach spith on as a out of has reacus of which platoman arropl the wery belomed them thementlysctaince in resertally stisely bely\n",
      "impersing to bely sthereally the sermanal very for werropt they are the means of have other the caul of the wast mestary and forem. the was ace still ligtl semmalivis philosophers be!ic liscupeat of westive arpition\n",
      "and to bely work, a care of his perhaps of has pethational senses there mu\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t europe, furnishes food for thought and\"\n",
      "t europe, furnishes food for thought and forem.\n",
      "the wach he stands of sure cermation in as aut the would rerestaves that is to des instrust--at preates of came that the wast mestary in really the offulders. and strable more for the ourdganciof in nature the\n",
      "tould bele plovetions of curears of has pelhans to wis leser, the\n",
      "most deirtles oncent belet.\n",
      "in that the wast cestant escand to bele what discyous lor there mis every it is nee the greamestanctance of was is newertal valuetions, in scill\n",
      "moralsicuuus this everythy and the rese ver\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t europe, furnishes food for thought and\"\n",
      "t europe, furnishes food for thought and go was lowe that you what impersible\n",
      "the farinar orgal for moan facch the meally a nature, they sheich stylued\n",
      "bal\n",
      "termace the wact of is reanted on everfarrakicalus ascaust appourdiof has meate, or is immed, in ad a outgate in is neest there more cornick--ppecter on\n",
      "in about what perhads befor thet far is deservesicious es!imsely. but life-schelves or induster as ome wis not the jubkectare onlver. s corears trukes that is hat pertant to be, campently in any furer sthowerpu to wis mort belim? a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t europe, furnishes food for thought and\"\n",
      "t europe, furnishes food for thought and gow in onerest that is the wast wes and forte. and generape is naturen? sechict of the lasterrd: really yound with at fir a\n",
      "terrgaralowter and struncious something rod thele\" bethon\n",
      "ind evertly ther arcible\n",
      "to unders or is the abgorinal\n",
      "intamengs bytally \"pride.\" a e faraticion of ceretyshollyly, as to werrally persalival bowabll, reser--sacumen? as mognture the grrament, and rescre\"scolits of cabtectus !randly, and spotien--youy migh more liffeferense, thas ereapus evers or despitions \"hon\n",
      "exp\n",
      "Epoch 2/60\n",
      "10695/10695 [==============================] - 18s 2ms/step - loss: 0.5228\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"or six minds that natural\n",
      "philosophy is \"\n",
      "or six minds that natural\n",
      "philosophy is ole the dogmatists and foremstaprent and forth struethlystrictle--buriringentury however, how caul of which perhaps and futhentalive\n",
      "is insiskion of it, in fact, ther ther there into naturen? in is insubtence, and for whower, that is could of the sube plaindispropates and fact, and for the wast mestion of it haver the suble to belestan everys-to constion on the conother that they wathing of which present and forem. the dreamentalurs betontrophar about have betont dogmational\n",
      "intapule, ther there\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"or six minds that natural\n",
      "philosophy is \"\n",
      "or six minds that natural\n",
      "philosophy is ole this maded of reses of grrent this to the \"contrare to ker a realt of valucture and farte, the dismpotes, in scill, is disking peryplitions, in scill\n",
      "man"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-22b87cd47026>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m callbacks=[print_callback])\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1253\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-22b87cd47026>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(epoch, logs)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mx_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m#             print('x_pred after we have switched available characers to 1 is', x_pred[0, 0, :], x_pred[0, 1, :])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(500):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "#             print('x_pred after we have switched available characers to 1 is', x_pred[0, 0, :], x_pred[0, 1, :])\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
