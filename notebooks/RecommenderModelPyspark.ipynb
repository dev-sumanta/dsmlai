{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-095e918bd4a8>:23 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-095e918bd4a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark220hdp27\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mD:\\spark220hdp27\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 299\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-095e918bd4a8>:23 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import findspark \n",
    "\n",
    "findspark.init()\n",
    "# set spark_path to the location where you have extracted spark\n",
    "spark_path = \"D:\\spark220hdp27\"\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_path\n",
    "os.environ[\"HADOOP_HOME\"] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConfx\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = SparkContext()\n",
    "print(sc.version)\n",
    "print(spark.version)\n",
    "print(sc.version)\n",
    "# do not run this cell again as sparkcntext has been initialized\n",
    "# and running it again is asking for multiple spark contexts which`\n",
    "# will raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   hello|\n",
      "+--------+\n",
      "|whatever|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'whatever' as hello\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkContext version  2.2.0\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print('sparkContext version ', sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting Movie Ratings**\n",
    "#### One of the most common uses of big data is to predict what users want.  This allows Google to show  us relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that we may like.  We will demonstrate how we can use Apache Spark to recommend movies to a user.  We will start with some basic techniques, and then use the [Spark MLlib][mllib] library's Alternating Least Squares method to make more sophisticated predictions.\n",
    "#### We will use a subset dataset of 500,000 ratings available on Databricks from the [movielens 10M stable benchmark rating dataset](http://grouplens.org/datasets/movielens/). However, the same code will work for the full dataset, or their latest dataset of 21 million ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ratingsFilename = 'ratings.dat.gz'\n",
    "moviesFilename = 'movies.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 487650 ratings and 3883 movies in the datasets\n",
      "Ratings: [(1, 1193, 5.0), (1, 661, 3.0), (1, 914, 3.0)]\n",
      "Movies: [(1, 'Toy Story (1995)'), (2, 'Jumanji (1995)'), (3, 'Grumpier Old Men (1995)')]\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 2\n",
    "rawRatings = sc.textFile(ratingsFilename).repartition(numPartitions)\n",
    "rawMovies = sc.textFile(moviesFilename)\n",
    "\n",
    "def get_ratings_tuple(entry):\n",
    "    \"\"\" Parse a line in the ratings dataset\n",
    "    Args:\n",
    "        entry (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp\n",
    "    Returns:\n",
    "        tuple: (UserID, MovieID, Rating)\n",
    "    \"\"\"\n",
    "    items = entry.split('::')\n",
    "    return int(items[0]), int(items[1]), float(items[2])\n",
    "\n",
    "\n",
    "def get_movie_tuple(entry):\n",
    "    \"\"\" Parse a line in the movies dataset\n",
    "    Args:\n",
    "        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres\n",
    "    Returns:\n",
    "        tuple: (MovieID, Title)\n",
    "    \"\"\"\n",
    "    items = entry.split('::')\n",
    "    return int(items[0]), items[1]\n",
    "\n",
    "\n",
    "ratingsRDD = rawRatings.map(get_ratings_tuple).cache()\n",
    "moviesRDD = rawMovies.map(get_movie_tuple).cache()\n",
    "\n",
    "ratingsCount = ratingsRDD.count()\n",
    "moviesCount = moviesRDD.count()\n",
    "\n",
    "print ('There are %s ratings and %s movies in the datasets' % (ratingsCount, moviesCount))\n",
    "print ('Ratings: %s' % ratingsRDD.take(3))\n",
    "print ('Movies: %s' % moviesRDD.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n",
      "3615\n"
     ]
    }
   ],
   "source": [
    "print( ratingsRDD.map(lambda x : x[0]).distinct().count())\n",
    "print( ratingsRDD.map(lambda x : x[1]).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Basic Recommendations**\n",
    "#### One way to recommend movies is to always recommend the movies with the highest average rating. In this part, we will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and at least 500 reviews. We want to filter our movies with high ratings but fewer than 500 reviews because movies with few reviews may not have broad appeal to everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1a) Number of Ratings and Average Ratings for a Movie\n",
    "Using only Python, implement a helper function getCountsAndAverages() that takes a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...)) and returns a tuple of (MovieID, (number of ratings, averageRating)). For example, given the tuple (100, (10.0, 20.0, 30.0)), your function should return (100, (3, 20.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, implement a helper function `getCountsAndAverages` using only Python\n",
    "def getCountsAndAverages(IDandRatingsTuple):\n",
    "    \"\"\" Calculate average rating\n",
    "    Args:\n",
    "        IDandRatingsTuple: a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...))\n",
    "    Returns:\n",
    "        tuple: a tuple of (MovieID, (number of ratings, averageRating))\n",
    "    \"\"\"\n",
    "    x = IDandRatingsTuple\n",
    "    return (x[0],(len(x[1]),float(sum(x[1]))/len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert getCountsAndAverages((1, (1, 2, 3, 4)) ) == (1,(4,2.5)),\\\n",
    "'incorrect count and average'\n",
    "assert getCountsAndAverages((100, (10.0, 20.0, 30.0))) == (100, (3, 20.0)),\\\n",
    "'incorrect getCountsAndAverages() with float list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1b) Movies with Highest Average Ratings**\n",
    "#### Now that we have a way to calculate the average ratings, we will use the `getCountsAndAverages()` helper function with Spark to determine movies with highest average ratings.\n",
    "#### The steps:\n",
    "* #### Recall that the `ratingsRDD` contains tuples of the form (UserID, MovieID, Rating). From `ratingsRDD` create an RDD with tuples of the form (MovieID, Python iterable of Ratings for that MovieID). This transformation will yield an RDD of the form: `[(1, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7c90>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e79d0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7610>)]`. Note that we will only need to perform two Spark transformations to do this step.\n",
    "* #### Using `movieIDsWithRatingsRDD` and our `getCountsAndAverages()` helper function, compute the number of ratings and average rating for each movie to yield tuples of the form (MovieID, (number of ratings, average rating)). This transformation will yield an RDD of the form: `[(1, (993, 4.145015105740181)), (2, (332, 3.174698795180723)), (3, (299, 3.0468227424749164))]`. We can do this step with one Spark transformation\n",
    "* #### We want to see movie names, instead of movie IDs. To `moviesRDD`, apply RDD transformations that use `movieIDsWithAvgRatingsRDD` to get the movie names for `movieIDsWithAvgRatingsRDD`, yielding tuples of the form (average rating, movie name, number of ratings). This set of transformations will yield an RDD of the form: `[(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1), (1.0, u'Big Squeeze, The (1996)', 3)]`. \n",
    "* #### We will need to do two Spark transformations to complete this step: first use the `moviesRDD` with `movieIDsWithAvgRatingsRDD` to create a new RDD with Movie names matched to Movie IDs, then convert that RDD into the form of (average rating, movie name, number of ratings). These transformations will yield an RDD that looks like: `[(3.6818181818181817, u'Happiest Millionaire, The (1967)', 22), (3.0468227424749164, u'Grumpier Old Men (1995)', 299), (2.882978723404255, u'Hocus Pocus (1993)', 94)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieIDsWithRatingsRDD: [(914, <pyspark.resultiterable.ResultIterable object at 0x00000280696A10B8>), (3408, <pyspark.resultiterable.ResultIterable object at 0x00000280696A1BE0>), (2804, <pyspark.resultiterable.ResultIterable object at 0x00000280696A1DD8>)]\n",
      "\n",
      "movieIDsWithAvgRatingsRDD: [(914, (314, 4.156050955414012)), (3408, (735, 3.8190476190476192)), (2804, (662, 4.2250755287009065))]\n",
      "\n",
      "movieNameWithAvgRatingsRDD: [(2.676056338028169, 'Waiting to Exhale (1995)', 71), (2.926829268292683, 'Tom and Huck (1995)', 41), (2.3777777777777778, 'Dracula: Dead and Loving It (1995)', 90)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From ratingsRDD with tuples of (UserID, MovieID, Rating) create an RDD with tuples of\n",
    "# the (MovieID, iterable of Ratings for that MovieID)\n",
    "movieIDsWithRatingsRDD = (ratingsRDD\n",
    "                          .map(lambda x:(x[1],x[2])).groupByKey())\n",
    "print ('movieIDsWithRatingsRDD: %s\\n' % movieIDsWithRatingsRDD.take(3))\n",
    "\n",
    "# Using `movieIDsWithRatingsRDD`, compute the number of ratings and average rating for each movie to\n",
    "# yield tuples of the form (MovieID, (number of ratings, average rating))\n",
    "movieIDsWithAvgRatingsRDD = movieIDsWithRatingsRDD.map(lambda x:getCountsAndAverages(x))\n",
    "print ('movieIDsWithAvgRatingsRDD: %s\\n' % movieIDsWithAvgRatingsRDD.take(3))\n",
    "\n",
    "# To `movieIDsWithAvgRatingsRDD`, apply RDD transformations that use `moviesRDD` to get the movie\n",
    "# names for `movieIDsWithAvgRatingsRDD`, yielding tuples of the form\n",
    "# (average rating, movie name, number of ratings)\n",
    "movieNameWithAvgRatingsRDD = (moviesRDD\n",
    "                            .join(movieIDsWithAvgRatingsRDD)).map(lambda x:(x[1][1][1],x[1][0],x[1][1][0]))  \n",
    "print ('movieNameWithAvgRatingsRDD: %s\\n' % movieNameWithAvgRatingsRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Movies with Highest Average Ratings and more than 500 reviews**\n",
    "#### Now that we have an RDD of the movies with highest averge ratings, we can use Spark to determine the 20 movies with highest average ratings and at least 500 reviews.\n",
    "#### Apply a single RDD transformation to `movieNameWithAvgRatingsRDD` to limit the results to movies with ratings from more than 500 people. We then use the `sortFunction()` helper function to sort by the average rating to get the movies in order of their rating (highest rating first). We will end up with an RDD of the form: `[(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088), (4.515798462852263, u\"Schindler's List (1993)\", 1171), (4.512893982808023, u'Godfather, The (1972)', 1047)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert movieIDsWithRatingsRDD.count() == 3615, \\\n",
    "        'incorrect movieIDsWithRatingsRDD.count() (expected 3615)'\n",
    "movieIDsWithRatingsTakeOrdered = movieIDsWithRatingsRDD.takeOrdered(3)\n",
    "\n",
    "assert movieIDsWithRatingsTakeOrdered[0][0] == 1 and \\\n",
    "len(list(movieIDsWithRatingsTakeOrdered[0][1])) == 993,\\\n",
    "'incorrect count of ratings for movieIDsWithRatingsTakeOrdered[0] (expected 993)'\n",
    "\n",
    "assert movieIDsWithRatingsTakeOrdered[1][0] == 2 and \\\n",
    "len(list(movieIDsWithRatingsTakeOrdered[1][1])) == 332,\\\n",
    "'incorrect count of ratings for movieIDsWithRatingsTakeOrdered[1] (expected 332)'\n",
    "\n",
    "assert movieIDsWithRatingsTakeOrdered[2][0] == 3 and \\\n",
    "len(list(movieIDsWithRatingsTakeOrdered[2][1])) == 299, \\\n",
    "'incorrect count of ratings for movieIDsWithRatingsTakeOrdered[2] (expected 299)'\n",
    "\n",
    "assert movieIDsWithAvgRatingsRDD.takeOrdered(3) == \\\n",
    "[(1, (993, 4.145015105740181)), (2, (332, 3.174698795180723)),\n",
    " (3, (299, 3.0468227424749164))], \\\n",
    "'incorrect movieIDsWithAvgRatingsRDD.takeOrdered(3)'\n",
    "assert movieNameWithAvgRatingsRDD.count() == 3615, \\\n",
    "                'incorrect movieNameWithAvgRatingsRDD.count() (expected 3615)'\n",
    "assert movieNameWithAvgRatingsRDD.takeOrdered(3) == \\\n",
    "[(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1),\n",
    "(1.0, u'Big Squeeze, The (1996)', 3)], \\\n",
    " 'incorrect movieNameWithAvgRatingsRDD.takeOrdered(3)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortFunction(tuple):\n",
    "    \"\"\" Construct the sort string (does not perform actual sorting)\n",
    "    Args:\n",
    "        tuple: (rating, MovieName)\n",
    "    Returns:\n",
    "        sortString: the value to sort with, 'rating MovieName'\n",
    "    \"\"\"\n",
    "    key = unicode('%.3f' % tuple[0])\n",
    "    value = tuple[1]\n",
    "    return (key + ' ' + value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.921985815602837, 'Twelve Monkeys (1995)', 705),\n",
       " (4.43953006219765, 'Star Wars: Episode IV - A New Hope (1977)', 1447),\n",
       " (4.296438883541867, 'Pulp Fiction (1994)', 1039),\n",
       " (3.453846153846154, 'Stargate (1994)', 520),\n",
       " (4.121270452358036, 'Forrest Gump (1994)', 1039)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieNameWithAvgRatingsRDD.filter(lambda x: x[2] > 500).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with highest ratings: [(4.5349264705882355, 'Shawshank Redemption, The (1994)', 1088), (4.515798462852263, \"Schindler's List (1993)\", 1171), (4.512893982808023, 'Godfather, The (1972)', 1047), (4.510460251046025, 'Raiders of the Lost Ark (1981)', 1195), (4.505415162454874, 'Usual Suspects, The (1995)', 831), (4.457256461232604, 'Rear Window (1954)', 503), (4.45468509984639, 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651), (4.43953006219765, 'Star Wars: Episode IV - A New Hope (1977)', 1447), (4.4, 'Sixth Sense, The (1999)', 1110), (4.394285714285714, 'North by Northwest (1959)', 700), (4.379506641366224, 'Citizen Kane (1941)', 527), (4.375, 'Casablanca (1942)', 776), (4.363975155279503, 'Godfather: Part II, The (1974)', 805), (4.358816276202219, \"One Flew Over the Cuckoo's Nest (1975)\", 811), (4.358173076923077, 'Silence of the Lambs, The (1991)', 1248), (4.335826477187734, 'Saving Private Ryan (1998)', 1337), (4.326241134751773, 'Chinatown (1974)', 564), (4.325383304940375, 'Life Is Beautiful (La Vita � bella) (1997)', 587), (4.324110671936759, 'Monty Python and the Holy Grail (1974)', 759), (4.3096, 'Matrix, The (1999)', 1250)]\n"
     ]
    }
   ],
   "source": [
    "# Apply an RDD transformation to `movieNameWithAvgRatingsRDD` to limit the results to movies with\n",
    "# ratings from more than 500 people. We then use the `sortFunction()` helper function to sort by the\n",
    "# average rating to get the movies in order of their rating (highest rating first)\n",
    "movieLimitedAndSortedByRatingRDD = (movieNameWithAvgRatingsRDD\n",
    "                                    .filter(lambda x: x[2] > 500)\n",
    "                                    .sortBy(lambda x : -x[0]))\n",
    "print ('Movies with highest ratings: %s' % movieLimitedAndSortedByRatingRDD.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert movieLimitedAndSortedByRatingRDD.count()  == 194,\\\n",
    "                'incorrect movieLimitedAndSortedByRatingRDD.count()'\n",
    "assert movieLimitedAndSortedByRatingRDD.take(20) == \\\n",
    "              [(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088),\n",
    "               (4.515798462852263, u\"Schindler's List (1993)\", 1171),\n",
    "               (4.512893982808023, u'Godfather, The (1972)', 1047),\n",
    "               (4.510460251046025, u'Raiders of the Lost Ark (1981)', 1195),\n",
    "               (4.505415162454874, u'Usual Suspects, The (1995)', 831),\n",
    "               (4.457256461232604, u'Rear Window (1954)', 503),\n",
    "               (4.45468509984639, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651),\n",
    "               (4.43953006219765, u'Star Wars: Episode IV - A New Hope (1977)', 1447),\n",
    "               (4.4, u'Sixth Sense, The (1999)', 1110), (4.394285714285714, u'North by Northwest (1959)', 700),\n",
    "               (4.379506641366224, u'Citizen Kane (1941)', 527), (4.375, u'Casablanca (1942)', 776),\n",
    "               (4.363975155279503, u'Godfather: Part II, The (1974)', 805),\n",
    "               (4.358816276202219, u\"One Flew Over the Cuckoo's Nest (1975)\", 811),\n",
    "               (4.358173076923077, u'Silence of the Lambs, The (1991)', 1248),\n",
    "               (4.335826477187734, u'Saving Private Ryan (1998)', 1337),\n",
    "               (4.326241134751773, u'Chinatown (1974)', 564),\n",
    "               (4.325383304940375, u'Life Is Beautiful (La Vita \\ufffd bella) (1997)', 587),\n",
    "               (4.324110671936759, u'Monty Python and the Holy Grail (1974)', 759),\n",
    "               (4.3096, u'Matrix, The (1999)', 1250)], 'incorrect sortedByRatingRDD.take(20)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: Collaborative Filtering**\n",
    "#### Spark exposes some higher level functionality; in particular, Machine Learning using a component of Spark called [MLlib][mllib].  \n",
    "#### We are going to use a technique called [collaborative filtering][collab]. Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. \n",
    "#### The image below (from [Wikipedia][collab]) shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image below the system has made a prediction, that the active user will not like the video.\n",
    "![collaborative filtering](https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since not all users have rated all movies, we do not know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n",
    "![factorization](http://spark-mooc.github.io/web-assets/images/matrix_factorization.png)\n",
    "#### We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The [Alternating Least Squares][als] algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constrant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n",
    "#### This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Creating a Training Set**\n",
    "#### Before we jump into using machine learning, we need to break up the `ratingsRDD` dataset into three pieces:\n",
    "* #### A training set (RDD), which we will use to train models\n",
    "* #### A validation set (RDD), which we will use to choose the best model\n",
    "* #### A test set (RDD), which we will use for our experiments\n",
    "#### To randomly split the dataset into the multiple groups, we can use the pySpark [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) transformation. `randomSplit()` takes a set of splits and and seed and returns multiple RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 293180, validation: 96898, test: 97572\n",
      "\n",
      "[(1, 1193, 5.0), (1, 661, 3.0), (1, 2355, 5.0)]\n",
      "[(1, 914, 3.0), (1, 3408, 4.0), (1, 2321, 3.0)]\n",
      "[(1, 1197, 3.0), (1, 1287, 5.0), (1, 2804, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "trainingRDD, validationRDD, testRDD = ratingsRDD.randomSplit([6, 2, 2], seed=0)\n",
    "\n",
    "print ('Training: %s, validation: %s, test: %s\\n' % (trainingRDD.count(),\n",
    "                                                    validationRDD.count(),\n",
    "                                                    testRDD.count()))\n",
    "print (trainingRDD.take(3))\n",
    "print (validationRDD.take(3))\n",
    "print (testRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Root Mean Square Error (RMSE)**\n",
    "####  We will use the [Root Mean Square Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) or Root Mean Square Deviation (RMSD) to compute the error of each model.  RMSE is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSE serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSE is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent.\n",
    "####  The RMSE is the square root of the average value of the square of `(actual rating - predicted rating)` for all users and movies for which we have the actual rating. Versions of Spark MLlib beginning with Spark 1.4 include a [RegressionMetrics](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RegressionMetrics) modiule that can be used to compute the RMSE. However, since we are using Spark 1.3.1, we will write our own function.\n",
    "#### Write a function to compute the sum of squared error given `predictedRDD` and `actualRDD` RDDs. Both RDDs consist of tuples of the form (UserID, MovieID, Rating)\n",
    "#### Given two ratings RDDs, *x* and *y* of size *n*, we define RSME as follows: $ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}$\n",
    "#### To calculate RSME, the steps to perform are:\n",
    "* #### Transform `predictedRDD` into the tuples of the form ((UserID, MovieID), Rating). For example, tuples like `[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]`. \n",
    "* #### Transform `actualRDD` into the tuples of the form ((UserID, MovieID), Rating). For example, tuples like `[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]`. \n",
    "* #### Using only RDD transformations, compute the squared error for each *matching* entry (i.e., the same (UserID, MovieID) in each RDD) in the reformatted RDDs. Note that not every (UserID, MovieID) pair will appear in both RDDs - if a pair does not appear in both RDDs, then it does not contribute to the RMSE. We will end up with an RDD with entries of the form $ (x_i - y_i)^2$ \n",
    "* #### Using an RDD action,compute the total squared error: $ SE = \\sum_{i = 1}^{n} (x_i - y_i)^2 $\n",
    "* #### Compute *n* by using an RDD action (but **not** `collect()`), to count the number of pairs for which we have computed the total squared error\n",
    "* #### Using the total squared error and the number of pairs, compute the RSME. We have to compute this value as a [float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for test dataset (should be 1.22474487139): 1.224744871391589\n",
      "Error for test dataset2 (should be 3.16227766017): 3.1622776601683795\n",
      "Error for testActual dataset (should be 0.0): 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import math\n",
    "\n",
    "def computeError(predictedRDD, actualRDD):\n",
    "    \"\"\" Compute the root mean squared error between predicted and actual\n",
    "    Args:\n",
    "        predictedRDD: predicted ratings for each movie and each user where each entry is in the form\n",
    "                      (UserID, MovieID, Rating)\n",
    "        actualRDD: actual ratings where each entry is in the form (UserID, MovieID, Rating)\n",
    "    Returns:\n",
    "        RSME (float): computed RSME value\n",
    "    \"\"\"\n",
    "    # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    predictedReformattedRDD = predictedRDD.map(lambda x:((x[0],x[1]),x[2]))\n",
    "   \n",
    "    # Transform actualRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    actualReformattedRDD = actualRDD.map(lambda x:((x[0],x[1]),x[2]))\n",
    " \n",
    "    # Compute the squared error for each matching entry (i.e., the same (User ID, Movie ID) in each\n",
    "    # RDD) in the reformatted RDDs using RDD transformtions - do not use collect()\n",
    "    squaredErrorsRDD = (predictedReformattedRDD\n",
    "                        .join(actualReformattedRDD)).map(lambda x:(x[0], \n",
    "                        pow(float((x[1][0])-float(x[1][1])),2)))\n",
    "    # Compute the total squared error - do not use collect()\n",
    "    totalError = squaredErrorsRDD.map(lambda x:(float(x[1]))).sum()\n",
    "\n",
    "    # Count the number of entries for which you computed the total squared error\n",
    "    numRatings = squaredErrorsRDD.count()\n",
    "   \n",
    "    # Using the total squared error and the number of entries, compute the RSME\n",
    "    return math.sqrt(float(totalError)/numRatings)\n",
    "\n",
    "\n",
    "# sc.parallelize turns a Python list into a Spark RDD.\n",
    "testPredicted = sc.parallelize([\n",
    "    (1, 1, 5),\n",
    "    (1, 2, 3),\n",
    "    (1, 3, 4),\n",
    "    (2, 1, 3),\n",
    "    (2, 2, 2),\n",
    "    (2, 3, 4)])\n",
    "testActual = sc.parallelize([\n",
    "     (1, 2, 3),\n",
    "     (1, 3, 5),\n",
    "     (2, 1, 5),\n",
    "     (2, 2, 1)])\n",
    "testPredicted2 = sc.parallelize([\n",
    "     (2, 2, 5),\n",
    "     (1, 2, 5)])\n",
    "testError = computeError(testPredicted, testActual)\n",
    "print ('Error for test dataset (should be 1.22474487139): %s' % testError)\n",
    "\n",
    "testError2 = computeError(testPredicted2, testActual)\n",
    "print ('Error for test dataset2 (should be 3.16227766017): %s' % testError2)\n",
    "\n",
    "testError3 = computeError(testActual, testActual)\n",
    "print ('Error for testActual dataset (should be 0.0): %s' % testError3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.0, 3.0), (4.0, 5.0), (2.0, 1.0)]\n",
      "1.224744871391589\n"
     ]
    }
   ],
   "source": [
    "# Using the canned RegressionMetrics from the pyspark.mllib.evaluation library\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "predictedReformattedRDD = testPredicted.map(lambda x:((x[0],x[1]),x[2]))\n",
    "actualReformattedRDD = testActual.map(lambda x:((x[0],x[1]),x[2]))\n",
    "actual_and_predicted = predictedReformattedRDD.join(actualReformattedRDD).map(\n",
    "lambda x : ( float(x[1][0]),float(x[1][1])))\n",
    "print(actual_and_predicted.take(3))\n",
    "arm = RegressionMetrics(actual_and_predicted)\n",
    "print(arm.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Using ALS.train()**\n",
    "#### In this part, we will use the MLlib implementation of Alternating Least Squares, [ALS.train()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS). ALS takes a training dataset (RDD) and several parameters that control the model creation process. To determine the best values for the parameters, we will use ALS to train several models, and then we will select the best model and use the parameters from that model in the rest of this lab exercise.\n",
    "#### The process we will use for determining the best model is as follows:\n",
    "* #### Pick a set of model parameters. The most important parameter to `ALS.train()` is the *rank*, which is the number of rows in the Users matrix (green in the diagram above) or the number of columns in the Movies matrix (blue in the diagram above). (In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting).)  We will train models with ranks of 4, 8, and 12 using the `trainingRDD` dataset.\n",
    "* #### Create a model using `ALS.train(trainingRDD, rank, seed=seed, iterations=iterations, lambda_=regularizationParameter)` with three parameters: an RDD consisting of tuples of the form (UserID, MovieID, rating) used to train the model, an integer rank (4, 8, or 12), a number of iterations to execute (we will use 5 for the `iterations` parameter), and a regularization coefficient (we will use 0.1 for the `regularizationParameter`).\n",
    "* #### For the prediction step, create an input RDD, `validationForPredictRDD`, consisting of (UserID, MovieID) pairs extracted from `validationRDD`. We will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* #### Using the model and `validationForPredictRDD`, we can predict rating values by calling [model.predictAll()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.predictAll) with the `validationForPredictRDD` dataset, where `model` is the model we generated with ALS.train().  `predictAll` accepts an RDD with each entry in the format (userID, movieID) and outputs an RDD with each entry in the format (userID, movieID, rating).\n",
    "* #### Evaluate the quality of the model by using the `computeError()` function we wrote in part (2b) to compute the error between the predicted ratings and the actual ratings in `validationRDD`.\n",
    "####  Which rank produces the best model, based on the RMSE with the `validationRDD` dataset?\n",
    "#### Note: It is likely that this operation will take a noticeable amount of time (around a minute in our VM); We can observe its progress on the [Spark Web UI](http://localhost:4040). Probably most of the time will be spent running your `computeError()` function, since, unlike the Spark ALS implementation (and the Spark 1.4 [RegressionMetrics](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RegressionMetrics) module), this does not use a fast linear algebra library and needs to run some Python code for all 100k entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.8923714161029934\n",
      "For rank 8 the RMSE is 0.8954064345072859\n",
      "For rank 12 the RMSE is 0.8912742324386047\n",
      "The best model was trained with rank 12\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "validationForPredictRDD = validationRDD.map(lambda x:(x[0],x[1]))\n",
    "\n",
    "seed = 5\n",
    "iterations = 5\n",
    "regularizationParameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "minError = float('inf')\n",
    "bestRank = -1\n",
    "bestIteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingRDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularizationParameter)\n",
    "    predictedRatingsRDD = model.predictAll(validationForPredictRDD)\n",
    "    error = computeError(predictedRatingsRDD, validationRDD)\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % bestRank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert trainingRDD.getNumPartitions()  == 2,\\\n",
    "                  'incorrect number of partitions for trainingRDD (expected 2)'\n",
    "assert validationForPredictRDD.count() == 96898,\\\n",
    "                  'incorrect size for validationForPredictRDD (expected 96898)'\n",
    "assert validationForPredictRDD.filter(lambda t: t == (1, 150)).count() == 1, \\\n",
    "                  'incorrect content for validationForPredictRDD'\n",
    "assert errors[0] - 0.883710109497 < tolerance, 'incorrect errors[0]'\n",
    "assert errors[1] - 0.878486305621 < tolerance, 'incorrect errors[1]'\n",
    "assert errors[2] - 0.876832795659 < tolerance, 'incorrect errors[2]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2d) Testing Your Model**\n",
    "#### So far, we used the `trainingRDD` and `validationRDD` datasets to select the best model.  Since we used these two datasets to determine what model is best, we cannot use them to test how good the model is - otherwise we would be very vulnerable to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  To decide how good our model is, we need to use the `testRDD` dataset.  We will use the `bestRank` we determined in part (2c) to create a model for predicting the ratings for the test dataset and then we will compute the RMSE.\n",
    "#### The steps we will perform are:\n",
    "* #### Train a model, using the `trainingRDD`, `bestRank` from part (2c), and the parameters we used in in part (2c): `seed=seed`, `iterations=iterations`, and `lambda_=regularizationParameter` - we will include **all** of the parameters.\n",
    "* #### For the prediction step, create an input RDD, `testForPredictRDD`, consisting of (UserID, MovieID) pairs that we extract from `testRDD`. We will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* #### Use [myModel.predictAll()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.predictAll) to predict rating values for the test dataset.\n",
    "* #### For validation, use the `testRDD`and your `computeError` function to compute the RMSE between `testRDD` and the `predictedTestRDD` from the model.\n",
    "* #### Evaluate the quality of the model by using the `computeError()` function we wrote in part (2b) to compute the error between the predicted ratings and the actual ratings in `testRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8928043392048276\n"
     ]
    }
   ],
   "source": [
    "myModel = ALS.train(trainingRDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularizationParameter)\n",
    "testForPredictingRDD = testRDD.map(lambda x:(x[0],x[1]))\n",
    "predictedTestRDD = myModel.predictAll(testForPredictingRDD)\n",
    "\n",
    "testRMSE = computeError(testRDD, predictedTestRDD)\n",
    "\n",
    "print ('The model had a RMSE on the test set of %s' % testRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert abs(testRMSE - 0.87809838344) < tolerance, 'incorrect testRMSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2e) Comparing Our Model**\n",
    "#### Looking at the RMSE for the results predicted by the model versus the values in the test set is one way to evalute the quality of our model. Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set.\n",
    "#### The steps:\n",
    "* #### Use the `trainingRDD` to compute the average rating across all movies in that training dataset.\n",
    "* #### Use the average rating just determined and the `testRDD` to create an RDD with entries of the form (userID, movieID, average rating).\n",
    "* #### Use the `testRDD` to create an RDD with entries of the form (userID, movieID, rating).\n",
    "* #### Use `computeError` function to compute the RMSE between the `testForRMSERDD` validation RDD that we just created and the `testForAvgRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating for movies in the training set is 3.571601064192646\n",
      "The RMSE on the average set is 1.1144120501597654\n"
     ]
    }
   ],
   "source": [
    "trainingAvgRating = trainingRDD.map(lambda x: x[2]).sum()/trainingRDD.count()\n",
    "print ('The average rating for movies in the training set is %s' % trainingAvgRating)\n",
    "\n",
    "testForAvgRDD = testRDD.map(lambda x:(x[0],x[1],trainingAvgRating))\n",
    "testForRMSERDD = testRDD.map(lambda x: x)\n",
    "testAvgRMSE = computeError(testForRMSERDD, testForAvgRDD)\n",
    "print ('The RMSE on the average set is %s' % testAvgRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert abs(trainingAvgRating - 3.571601) < 0.000001, \\\n",
    "                'incorrect trainingAvgRating (expected 3.571601064192646)'\n",
    "assert abs(testAvgRMSE - 1.114412) < 0.000001, \\\n",
    "                'incorrect testAvgRMSE (expected assert 1.1144120501597654)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1335955760308405\n",
      "[(4, array('d', [-0.6683604717254639, 0.6176535487174988, -0.9526308178901672, 0.7968285083770752, 1.2056505680084229, 0.31765681505203247, -0.1131814643740654, -0.12488371878862381, 1.5376293659210205, 0.4793440103530884, -0.5800007581710815, -0.44496551156044006]))]\n",
      "[(10, array('d', [-0.582744300365448, -0.003755934303626418, -0.428880512714386, 0.43934208154678345, 0.22229070961475372, 0.2934376001358032, -0.14676639437675476, -0.7182983756065369, 0.7954515218734741, 0.2937465310096741, -0.3217548131942749, 0.06804702430963516]))]\n",
      "3.1335955760308405\n"
     ]
    }
   ],
   "source": [
    "# Understand how the predictions are made\n",
    "# Each row of UserFeatures is the inferred feature weights vector for the particular user\n",
    "# Each row of ProductFeatues is the inferred feature weights vector for the product\n",
    "print(myModel.predict(4,10))\n",
    "fourUser = myModel.userFeatures().filter( lambda x : x[0] == 4)\n",
    "print(fourUser.collect())\n",
    "tenMovie = myModel.productFeatures().filter(lambda x : x[0] == 10 )\n",
    "print(tenMovie.collect())\n",
    "fourTenRating = sum([ x[0] * x[1] for x in zip(fourUser.collect()[0][1],\n",
    "                                               tenMovie.collect()[0][1])])\n",
    "print(fourTenRating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Predictions for Yourself**\n",
    "#### The ultimate goal of this lab exercise is to predict what movies to recommend to oneself.  In order to do that, we will first need to add ratings for ourselves to the `ratingsRDD` dataset.\n",
    "\n",
    "### Let us see the most popular movies with number of ratings, names and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moviesMap = moviesRDD.map(lambda x : (x[1],x[0])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most rated movies:\n",
      "(number of ratings, (movie name, movie ID))\n",
      "1088 ('Shawshank Redemption, The (1994)', 318)\n",
      "1171 (\"Schindler's List (1993)\", 527)\n",
      "1047 ('Godfather, The (1972)', 858)\n",
      "1195 ('Raiders of the Lost Ark (1981)', 1198)\n",
      "831 ('Usual Suspects, The (1995)', 50)\n",
      "503 ('Rear Window (1954)', 904)\n",
      "651 ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 750)\n",
      "1447 ('Star Wars: Episode IV - A New Hope (1977)', 260)\n",
      "1110 ('Sixth Sense, The (1999)', 2762)\n",
      "700 ('North by Northwest (1959)', 908)\n",
      "527 ('Citizen Kane (1941)', 923)\n",
      "776 ('Casablanca (1942)', 912)\n",
      "805 ('Godfather: Part II, The (1974)', 1221)\n",
      "811 (\"One Flew Over the Cuckoo's Nest (1975)\", 1193)\n",
      "1248 ('Silence of the Lambs, The (1991)', 593)\n",
      "1337 ('Saving Private Ryan (1998)', 2028)\n",
      "564 ('Chinatown (1974)', 1252)\n",
      "587 ('Life Is Beautiful (La Vita � bella) (1997)', 2324)\n",
      "759 ('Monty Python and the Holy Grail (1974)', 1136)\n",
      "1250 ('Matrix, The (1999)', 2571)\n",
      "1438 ('Star Wars: Episode V - The Empire Strikes Back (1980)', 1196)\n",
      "553 ('Young Frankenstein (1974)', 1278)\n",
      "594 ('Psycho (1960)', 1219)\n",
      "1039 ('Pulp Fiction (1994)', 296)\n",
      "1218 ('Fargo (1996)', 608)\n",
      "811 ('GoodFellas (1990)', 1213)\n",
      "1775 ('American Beauty (1999)', 2858)\n",
      "817 ('Wizard of Oz, The (1939)', 919)\n",
      "1083 ('Princess Bride, The (1987)', 1197)\n",
      "600 ('Graduate, The (1967)', 1247)\n",
      "546 ('Run Lola Run (Lola rennt) (1998)', 2692)\n",
      "633 ('Amadeus (1984)', 1225)\n",
      "516 ('This Is Spinal Tap (1984)', 1288)\n",
      "860 ('Toy Story 2 (1999)', 3114)\n",
      "744 ('Almost Famous (2000)', 3897)\n",
      "662 ('Christmas Story, A (1983)', 2804)\n",
      "549 ('Glory (1989)', 1242)\n",
      "539 ('Apocalypse Now (1979)', 1208)\n",
      "1129 ('L.A. Confidential (1997)', 1617)\n",
      "845 ('Blade Runner (1982)', 541)\n",
      "562 ('Sling Blade (1996)', 1358)\n",
      "1300 ('Braveheart (1995)', 110)\n",
      "619 ('Butch Cassidy and the Sundance Kid (1969)', 1304)\n",
      "789 ('Good Will Hunting (1997)', 1704)\n",
      "551 ('Taxi Driver (1976)', 111)\n",
      "983 ('Terminator, The (1984)', 1240)\n",
      "603 ('Reservoir Dogs (1992)', 1089)\n",
      "750 ('Jaws (1975)', 1387)\n",
      "941 ('Alien (1979)', 1214)\n",
      "993 ('Toy Story (1995)', 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Most rated movies:')\n",
    "print ('(number of ratings, (movie name, movie ID))')\n",
    "# movieRatingsIDAndName = moveLimitedAndSortedByRatingRDD.\n",
    "for ratingsTuple in movieLimitedAndSortedByRatingRDD.take(50):\n",
    "    print( ratingsTuple[2],(ratingsTuple[1],moviesMap[ratingsTuple[1]]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The user ID 0 is unassigned, so we will use it for our ratings. We set the variable `myUserID` to 0  Next, create a new RDD `myRatingsRDD` with our ratings for at least 10 movie ratings. Each entry should be formatted as `(myUserID, movieID, rating)` (i.e., each entry should be formatted in the same way as `trainingRDD`).  As in the original dataset, ratings should be between 1 and 5 (inclusive). If we have not seen at least 10 of these movies, we can increase the parameter passed to `take()` in the above cell until there are 10 movies that we have seen (or we can also guess what our rating would be for movies we have not seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 'Usual Suspects, The (1995)'), (318, 'Shawshank Redemption, The (1994)'), (527, \"Schindler's List (1993)\"), (600, 'Love and a .45 (1994)'), (699, 'To Cross the Rubicon (1991)'), (811, 'Bewegte Mann, Der (1994)'), (858, 'Godfather, The (1972)'), (1129, 'Escape from New York (1981)'), (1158, 'Here Comes Cookie (1935)'), (1447, \"Gridlock'd (1997)\")]\n",
      "My movie ratings: [(0, 318, 4), (0, 527, 4), (0, 858, 4), (0, 1158, 4), (0, 50, 5), (0, 1447, 3), (0, 811, 5), (0, 600, 2), (0, 1129, 4), (0, 699, 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "myUserID = 0\n",
    "print(moviesRDD.filter( lambda x : x[0] in [318,527,858,1158,50,1447,\n",
    "                                           811,600,1129,699]).collect())\n",
    "\n",
    "myRatedMovies = [\n",
    "(myUserID, 318,4),\n",
    "(myUserID, 527,4),\n",
    "(myUserID, 858,4),\n",
    "(myUserID, 1158,4),\n",
    "(myUserID, 50,5),\n",
    "(myUserID, 1447,3),\n",
    "(myUserID, 811,5),\n",
    "(myUserID, 600,2),\n",
    "(myUserID, 1129,4),\n",
    "(myUserID, 699,2)\n",
    "     # The format of each line is (myUserID, movie ID, your rating)\n",
    "     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\"\n",
    "     # a five rating, you would add the following line:\n",
    "     #   (myUserID, 260, 5),\n",
    "    ]\n",
    "myRatingsRDD = sc.parallelize(myRatedMovies)\n",
    "print ('My movie ratings: %s' % myRatingsRDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3b) Add our Movies to Training Dataset**\n",
    "#### Now that we have ratings for ourself, we need to add your ratings to the `training` dataset so that the model we train will incorporate your preferences.  Spark's [union()](http://spark.apache.org/docs/latest/api/python/pyspark.rdd.RDD-class.html#union) transformation combines two RDDs; use `union()` to create a new training dataset that includes our ratings and the data in the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset now has 10 more entries than the original training dataset\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "trainingWithMyRatingsRDD = trainingRDD.union(myRatingsRDD)\n",
    "\n",
    "print ('The training dataset now has %s more entries than the original training dataset' %\n",
    "       (trainingWithMyRatingsRDD.count() - trainingRDD.count()))\n",
    "assert trainingWithMyRatingsRDD.count() - trainingRDD.count() == myRatingsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Train a Model with our Ratings**\n",
    "#### Now, train a model with our ratings added and the parameters we used in in part (2c): `bestRank`, `seed=seed`, `iterations=iterations`, and `lambda_=regularizationParameter` - make sure to include **all** of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularizationParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Check RMSE for the New Model with our Ratings**\n",
    "#### Compute the RMSE for this new model on the test set.\n",
    "* #### For the prediction step, we reuse `testForPredictRDD`, consisting of (UserID, MovieID) pairs that we extracted from `testRDD`. The RDD has the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* #### Use `myRatingsModel.predictAll()` to predict rating values for the `testForPredictRDD` test dataset, set this as `predictedTestMyRatingsRDD`\n",
    "* #### For validation, use the `testRDD`and our `computeError` function to compute the RMSE between `testRDD` and the `predictedTestMyRatingsRDD` from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8920077469667276\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "predictedTestMyRatingsRDD = myRatingsModel.predictAll(testForPredictingRDD)\n",
    "testRMSEMyRatings = computeError(testRDD,predictedTestMyRatingsRDD)\n",
    "print ('The model had a RMSE on the test set of %s' % testRMSEMyRatings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) Predict Our Ratings**\n",
    "#### So far, we have only used the `predictAll` method to compute the error of the model.  Here, we use the `predictAll` to predict what ratings we would give to the movies that you did not already provide ratings for.\n",
    "#### The steps:\n",
    "* #### Use the Python list `myRatedMovies` to transform the `moviesRDD` into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that we have rated. This transformation will yield an RDD of the form: `[(0, 1), (0, 2), (0, 3), (0, 4)]`.\n",
    "* #### For the prediction step, use the input RDD, `myUnratedMoviesRDD`, with myRatingsModel.predictAll() to predict our ratings for the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Toy Story (1995)'), (2, 'Jumanji (1995)'), (3, 'Grumpier Old Men (1995)')]\n",
      "[(0, 318, 4), (0, 527, 4), (0, 858, 4), (0, 1158, 4), (0, 50, 5), (0, 1447, 3), (0, 811, 5), (0, 600, 2), (0, 1129, 4), (0, 699, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(moviesRDD.take(3))\n",
    "print(myRatedMovies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the Python list myRatedMovies to transform the moviesRDD into an RDD with entries that are pairs of the form \n",
    "# (myUserID, Movie ID) and that does not contain any movies that we have rated.\n",
    "myUnratedMoviesRDD = (moviesRDD\n",
    "                      .filter(lambda x:[x[0] != [y[1] for y in myRatedMovies]])).map(\n",
    "    lambda x:(myUserID,x[0]))\n",
    "\n",
    "# Use the input RDD, myUnratedMoviesRDD, with myRatingsModel.predictAll() to predict our ratings for the movies\n",
    "predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=1084, rating=3.7912112276586187),\n",
       " Rating(user=0, product=3456, rating=3.116877299732263),\n",
       " Rating(user=0, product=3764, rating=2.1358960685234214),\n",
       " Rating(user=0, product=3272, rating=3.289825991446695),\n",
       " Rating(user=0, product=428, rating=3.601721794812863),\n",
       " Rating(user=0, product=1900, rating=2.7345154087855703),\n",
       " Rating(user=0, product=1328, rating=2.0423276968313004),\n",
       " Rating(user=0, product=464, rating=2.988989953889424),\n",
       " Rating(user=0, product=1336, rating=2.0597167230511713),\n",
       " Rating(user=0, product=912, rating=3.933520087436215)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedRatingsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3f) Predict Your Ratings**\n",
    "#### We have our predicted ratings. Now we can print out the 25 movies with the highest predicted ratings.\n",
    "#### The steps you should perform are:\n",
    "* #### From Parts (1b) and (1c), we know that we should look at movies with a reasonable number of reviews (e.g., more than 75 reviews). We can experiment with a lower threshold, but fewer ratings for a movie may yield higher prediction errors. Transform `movieIDsWithAvgRatingsRDD` from Part (1b), which has the form (MovieID, (number of ratings, average rating)), into an RDD of the form (MovieID, number of ratings): `[(2, 332), (4, 71), (6, 442)]`\n",
    "* #### We want to see movie names, instead of movie IDs. Transform `predictedRatingsRDD` into an RDD with entries that are pairs of the form (Movie ID, Predicted Rating): `[(3456, -0.5501005376936687), (1080, 1.5885892024487962), (320, -3.7952255522487865)]`\n",
    "* #### Use RDD transformations with `predictedRDD` and `movieCountsRDD` to yield an RDD with tuples of the form (Movie ID, (Predicted Rating, number of ratings)): `[(2050, (0.6694097486155939, 44)), (10, (5.29762541533513, 418)), (2060, (0.5055259373841172, 97))]`\n",
    "* #### Use RDD transformations with `predictedWithCountsRDD` and `moviesRDD` to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings), _for movies with more than 75 ratings._ For example: `[(7.983121900375243, u'Under Siege (1992)'), (7.9769201864261285, u'Fifth Element, The (1997)')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(914, 314), (3408, 735)]\n",
      "[(1084, 3.7912112276586187), (3456, 3.116877299732263)]\n",
      "[(3456, (3.116877299732263, 32)), (912, (3.933520087436215, 776))]\n",
      "My highest rated movies as predicted (for movies with more than 75 reviews):\n",
      "(4.429178504032896, '2001: A Space Odyssey (1968)', 811)\n",
      "(4.409992561242355, 'Alien (1979)', 941)\n",
      "(4.384725604951456, 'Blade Runner (1982)', 845)\n",
      "(4.359801417823164, 'Aliens (1986)', 832)\n",
      "(4.355473169212698, 'Star Wars: Episode IV - A New Hope (1977)', 1447)\n",
      "(4.350864366078438, 'Godfather, The (1972)', 1047)\n",
      "(4.3239603368480966, 'Lawrence of Arabia (1962)', 402)\n",
      "(4.296675765818163, 'Silence of the Lambs, The (1991)', 1248)\n",
      "(4.2965038296550455, 'Raiders of the Lost Ark (1981)', 1195)\n",
      "(4.268855057588508, 'Matrix, The (1999)', 1250)\n",
      "(4.262010359939317, 'Star Wars: Episode V - The Empire Strikes Back (1980)', 1438)\n",
      "(4.2527804503102455, 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)', 278)\n",
      "(4.248588504665106, 'Yojimbo (1961)', 110)\n",
      "(4.238349406954924, 'Saving Private Ryan (1998)', 1337)\n",
      "(4.229080029655405, 'Boat, The (Das Boot) (1981)', 483)\n",
      "(4.220771351152686, 'Stop Making Sense (1984)', 97)\n",
      "(4.215250327630421, 'Brazil (1985)', 416)\n",
      "(4.212518559399289, 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651)\n",
      "(4.197315808243679, 'Apocalypse Now (1979)', 539)\n",
      "(4.185331423838814, 'Pulp Fiction (1994)', 1039)\n"
     ]
    }
   ],
   "source": [
    "# Transform movieIDsWithAvgRatingsRDD from part (1b), which has the form\n",
    "# (MovieID, (number of ratings, average rating)), into and RDD of the form \n",
    "# (MovieID, number of ratings)\n",
    "movieCountsRDD = movieIDsWithAvgRatingsRDD.map(lambda x:(x[0],x[1][0]))\n",
    "print(movieCountsRDD.take(2))\n",
    "\n",
    "# Transform predictedRatingsRDD into an RDD with entries that are pairs of the form \n",
    "# (Movie ID, Predicted Rating)\n",
    "predictedRDD = predictedRatingsRDD.map(lambda x:(x[1],x[2]))\n",
    "print(predictedRDD.take(2))\n",
    "\n",
    "# Use RDD transformations with predictedRDD and movieCountsRDD to yield an RDD \n",
    "# with tuples of the form (Movie ID, (Predicted Rating, number of ratings))\n",
    "predictedWithCountsRDD  = (predictedRDD\n",
    "                           .join(movieCountsRDD))\n",
    "print(predictedWithCountsRDD.take(2))\n",
    "\n",
    "# Use RDD transformations with PredictedWithCountsRDD and moviesRDD to yield an RDD \n",
    "# with tuples of the form (Predicted Rating, Movie Name, number of ratings),\n",
    "# for movies with more than 75 ratings\n",
    "ratingsWithNamesRDD = (predictedWithCountsRDD\n",
    "                       .filter(lambda x: x[1][1] > 75)).join(moviesRDD).map(\n",
    "    lambda x:(x[1][0][0],x[1][1],x[1][0][1]))\n",
    "\n",
    "predictedHighestRatedMovies = ratingsWithNamesRDD.takeOrdered(20, key=lambda x: -x[0])\n",
    "print ('My highest rated movies as predicted (for movies with more than 75 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, predictedHighestRatedMovies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Item similarities\n",
    "### We can recommend products based on similarties between them. Having carried out the matrix factorization we have the product features available for all the movies in the movie set\n",
    "### The steps that we will follow to compute similarities for a particular movie would be \n",
    "### Devise a formula to compare vectors - we will use cosine similarity - the ratio of the dot product to the product of the norms of the vectors\n",
    "### a. Filter the product features to the movie of interest\n",
    "### b. Cross join it with the full product features set and get similarities for each pair\n",
    "### c. Create means to get movie ids and names together\n",
    "### d. Sort on similarities descending to get the item recommnedations based on item similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create a simple vector and see how we can get the dot product\n",
    "simpv = [3,3]\n",
    "sum( simpv[x] * simpv[x] for x in range(len(simpv))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets create a function that we will plug in to calculate the cosine similarities\n",
    "def cos_sim(f,s):\n",
    "    from math import sqrt\n",
    "    dot_product = sum([f[x] * s[x] for x in range(len(f))])\n",
    "    print(dot_product)\n",
    "    norms_product = sqrt( sum( x * x for x in f) ) * sqrt( sum(x * x for x in s))\n",
    "    print(norms_product)\n",
    "    return dot_product/norms_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "15.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the cosine similarity formula\n",
    "cos_sim([3,0],[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('d', [-0.20404702425003052, 0.4111984074115753, -0.8286676406860352, 0.418993204832077, 0.2826428711414337, 0.7812122702598572, -0.3807153105735779, -0.18545562028884888, 0.23406963050365448, 0.0689709261059761, -0.3330400288105011, 0.07071979343891144])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at the product features\n",
    "model.productFeatures().first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a movies dictionary so that we can comfortably get movie names from ids\n",
    "movies_map = moviesRDD.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2858, 'American Beauty (1999)', 1775),\n",
       " (260, 'Star Wars: Episode IV - A New Hope (1977)', 1447),\n",
       " (1196, 'Star Wars: Episode V - The Empire Strikes Back (1980)', 1438),\n",
       " (480, 'Jurassic Park (1993)', 1423),\n",
       " (1210, 'Star Wars: Episode VI - Return of the Jedi (1983)', 1390)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the most rated movies\n",
    "movieCountsRDD.sortBy(lambda x : -x[1]).map( lambda x : (x[0],movies_map[x[0]],x[1])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3386,\n",
       "  array('d', [-0.35212433338165283, -0.08273603022098541, -0.3896622061729431, 0.5738535523414612, 0.5002975463867188, 0.6489229202270508, -0.09155140817165375, -0.35801205039024353, 0.9639317393302917, 0.3876153826713562, -0.3232026696205139, -0.1436186134815216]))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets collect the product features for a particular movie\n",
    "movie_of_interest_id = 3386\n",
    "mi_pf = model.productFeatures().filter(lambda x : x[0] == movie_of_interest_id)\n",
    "mi_pf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3386, 'JFK (1991)', 1.0),\n",
       " (1466, 'Donnie Brasco (1997)', 0.9781235133055158),\n",
       " (1343, 'Cape Fear (1991)', 0.9747616463577208),\n",
       " (1082, 'Candidate, The (1972)', 0.9725997033804972),\n",
       " (98, 'Shopping (1994)', 0.9674208860310151),\n",
       " (475, 'In the Name of the Father (1993)', 0.965667025224523),\n",
       " (108, 'Catwalk (1995)', 0.9644767361391873),\n",
       " (2989, 'For Your Eyes Only (1981)', 0.9630112607491665),\n",
       " (2745, 'Mission, The (1986)', 0.962785087996856),\n",
       " (3020, 'Falling Down (1993)', 0.9627160790910411),\n",
       " (3050, 'Light It Up (1999)', 0.9616743720574553),\n",
       " (3001, 'Suburbans, The (1999)', 0.9579631970386354),\n",
       " (1384, 'Substance of Fire, The (1996)', 0.9578912101231464),\n",
       " (3852, 'Tao of Steve, The (2000)', 0.9572181418567901),\n",
       " (3263, \"White Men Can't Jump (1992)\", 0.9568034032965055),\n",
       " (2866, 'Buddy Holly Story, The (1978)', 0.9558337355447672),\n",
       " (3614, 'Honeymoon in Vegas (1992)', 0.9556058221725713),\n",
       " (3203, 'Dead Calm (1989)', 0.9537977435761551),\n",
       " (1532, 'Sprung (1997)', 0.9528833746832107),\n",
       " (3291, 'Trois (2000)', 0.9527032077099975),\n",
       " (1810, 'Primary Colors (1998)', 0.9524141715359331),\n",
       " (3595, 'Held Up (2000)', 0.9518566138898369),\n",
       " (2563, 'Beauty (1998)', 0.9507063833255066),\n",
       " (1589, 'Cop Land (1997)', 0.9491744600628705),\n",
       " (3763, 'F/X (1986)', 0.9488647491400286),\n",
       " (717, 'Mouth to Mouth (Boca a boca) (1995)', 0.9488571319884788),\n",
       " (774, \"Wend Kuuni (God's Gift) (1982)\", 0.9488571319884788),\n",
       " (628, 'Primal Fear (1996)', 0.9486543224931099),\n",
       " (847, 'Big Squeeze, The (1996)', 0.9486056880954201),\n",
       " (3039, 'Trading Places (1983)', 0.9484059392536607),\n",
       " (3526, 'Parenthood (1989)', 0.9481511855448443),\n",
       " (1840, 'He Got Game (1998)', 0.9479669235120256),\n",
       " (1625, 'Game, The (1997)', 0.9477288567002641),\n",
       " (6, 'Heat (1995)', 0.9475570867990004),\n",
       " (3245, 'I Am Cuba (Soy Cuba/Ya Kuba) (1964)', 0.9471921215666179),\n",
       " (3056, 'Oxygen (1999)', 0.9468359803682852),\n",
       " (2278, 'Ronin (1998)', 0.9460567437397994),\n",
       " (431, \"Carlito's Way (1993)\", 0.9452825141510044),\n",
       " (3363, 'American Graffiti (1973)', 0.9440503349812596),\n",
       " (823, 'Collectionneuse, La (1967)', 0.9437734642873811),\n",
       " (1538, 'Second Jungle Book: Mowgli & Baloo, The (1997)', 0.9427101753787399),\n",
       " (3102, 'Jagged Edge (1985)', 0.9426064661738379),\n",
       " (2841, 'Stir of Echoes (1999)', 0.9423059300014094),\n",
       " (2172,\n",
       "  'Strike! (a.k.a. All I Wanna Do, The Hairy Bird) (1998)',\n",
       "  0.9416850609610455),\n",
       " (1321, 'American Werewolf in London, An (1981)', 0.9412666403255905),\n",
       " (593, 'Silence of the Lambs, The (1991)', 0.9412435871452074),\n",
       " (2638, \"Mummy's Tomb, The (1942)\", 0.9404182360403353),\n",
       " (428, 'Bronx Tale, A (1993)', 0.9400839431078652),\n",
       " (1346, 'Cat People (1982)', 0.9400311628327618),\n",
       " (2079, 'Kidnapped (1960)', 0.9390869812808443),\n",
       " (2955, 'Penitentiary II (1982)', 0.939033348444171),\n",
       " (2981, 'Brother, Can You Spare a Dime? (1975)', 0.9388759613900978),\n",
       " (2850, 'Public Access (1993)', 0.9385965519101427),\n",
       " (1891, 'Ugly, The (1997)', 0.9383879860518637),\n",
       " (2391, 'Simple Plan, A (1998)', 0.9381580595408177),\n",
       " (1773, 'Tokyo Fist (1995)', 0.938116697205561),\n",
       " (3119, 'Bay of Blood (Reazione a catena) (1971)', 0.9381166947672976),\n",
       " (1953, 'French Connection, The (1971)', 0.938082080519796),\n",
       " (456, 'Fresh (1994)', 0.9378418036003545),\n",
       " (3360, 'Hoosiers (1986)', 0.9376758334515934),\n",
       " (3472,\n",
       "  'Horror Hotel (a.k.a. The City of the Dead) (1960)',\n",
       "  0.9374867588166345),\n",
       " (1259, 'Stand by Me (1986)', 0.9373665182963349),\n",
       " (3421, 'Animal House (1978)', 0.9368743676507943),\n",
       " (3215, 'Voyage of the Damned (1976)', 0.9367900513445464),\n",
       " (2349, 'Mona Lisa (1986)', 0.9364940528992123),\n",
       " (2474, 'Color of Money, The (1986)', 0.9364861475478827),\n",
       " (1440, 'Amos & Andrew (1993)', 0.9360932828612557),\n",
       " (3003, 'Train of Life (Train De Vie) (1998)', 0.935993240109597),\n",
       " (3379, 'On the Beach (1959)', 0.9358963856102498),\n",
       " (3471, 'Close Encounters of the Third Kind (1977)', 0.93556408957667),\n",
       " (2357, 'Central Station (Central do Brasil) (1998)', 0.9354731576213657),\n",
       " (3671, 'Blazing Saddles (1974)', 0.93509060524443),\n",
       " (2869, 'Separation, The (La S�paration) (1994)', 0.934018547828581),\n",
       " (2940, 'Gilda (1946)', 0.9339947776556335),\n",
       " (964, 'Angel and the Badman (1947)', 0.9333227827653978),\n",
       " (3529, 'Postman Always Rings Twice, The (1981)', 0.9326959626833807),\n",
       " (2916, 'Total Recall (1990)', 0.9325470362554029),\n",
       " (3860, 'Opportunists, The (1999)', 0.9325069807491659),\n",
       " (1240, 'Terminator, The (1984)', 0.9323864904002341),\n",
       " (474, 'In the Line of Fire (1993)', 0.9320922324868052),\n",
       " (3635, 'Spy Who Loved Me, The (1977)', 0.9320530271692826),\n",
       " (2117, 'Nineteen Eighty-Four (1984)', 0.9313122757298385),\n",
       " (2457, 'Running Scared (1986)', 0.930846688074277),\n",
       " (1849, 'Prince Valiant (1997)', 0.9299719308047674),\n",
       " (507, 'Perfect World, A (1993)', 0.9299274638482604),\n",
       " (1553, 'Late Bloomers (1996)', 0.9298501544294626),\n",
       " (1370, 'Die Hard 2 (1990)', 0.9297502318257953),\n",
       " (58, 'Postino, Il (The Postman) (1994)', 0.9296322798057046),\n",
       " (1298, 'Pink Floyd - The Wall (1982)', 0.9295801232257404),\n",
       " (391, \"Jason's Lyric (1994)\", 0.9295084504281996),\n",
       " (2183, 'Man Who Knew Too Much, The (1956)', 0.9292873961590029),\n",
       " (632, 'Land and Freedom (Tierra y libertad) (1995)', 0.9291683439147576),\n",
       " (3071, 'Stand and Deliver (1987)', 0.9290186624985837),\n",
       " (224, 'Don Juan DeMarco (1995)', 0.9288714739489851),\n",
       " (3218, 'Poison (1991)', 0.9286118137610698),\n",
       " (2231, 'Rounders (1998)', 0.9285967460594436),\n",
       " (3140, 'Three Ages, The (1923)', 0.9284604797373239),\n",
       " (556, 'War Room, The (1993)', 0.9283617084881104),\n",
       " (2685, 'Red Dwarf, The (Le Nain rouge) (1998)', 0.9282918345593529),\n",
       " (3069,\n",
       "  'Effect of Gamma Rays on Man-in-the-Moon Marigolds, The (1972)',\n",
       "  0.9276499092777829)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will do a cartesian of the movie of interest with the entire movie set\n",
    "# Apply the cosine similarity to the ratings\n",
    "# Map to movie names using movies_map\n",
    "# And sort descending on movie names to get the most similar movies\n",
    "mi_pf.cartesian(model.productFeatures()).map(lambda x : (x[0][0],x[1][0],cos_sim(x[0][1],x[1][1]) )).sortBy(\n",
    "lambda x : -x[2]).map( lambda x : (x[1],movies_map[x[1]],x[2])).take(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
