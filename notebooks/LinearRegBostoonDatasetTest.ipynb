{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testFunc(func,msg):\n",
    "    try:\n",
    "        func()\n",
    "    except AssertionError as e:\n",
    "        print ('fail', e)\n",
    "    else:\n",
    "        print (msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Boston datasets and take a look at the description\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print (boston.data.shape)\n",
    "print( boston.feature_names)\n",
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose features of X to be INDUS(2),RM(5),TAX(9) and PTRATIO(10) and LSTAT(11) \n",
    "#TODO - create the X matrix and the y target \n",
    "X = \n",
    "y = \n",
    "#TODO  - set Xmeans to the mean of the X design matrix and ymean to the mean of y\n",
    "Xmeans = \n",
    "ymean = \n",
    "def test_Xy_create():\n",
    "    assert Xmeans.tolist() == [11.13677865612648, 6.284634387351779, 408.2371541501976, 18.455533596837945,\n",
    "                               356.6740316205534],'' \\\n",
    "    'Check that you have used the correct indices to initialze and try again'\n",
    "    assert abs ( ymean - 22.53280 ) <= 0.00001 , 'Check the target vector y'\n",
    "\n",
    "\n",
    "testFunc(test_Xy_create,'The design matrix and the target vector created correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets use the standard scaler from sckikit preprocessing to normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#TODO - initialize scaler_X and scaler_y  using Standard Scaler and fit them to X and y respectively\n",
    "scaler_X = \n",
    "scaler_y = \n",
    "#TODO - set X_scaled and y_scaled to the transformed versions produced by using the transform method of scaler_x and scaler_y\n",
    "X_scaled = \n",
    "y_scaled = \n",
    "#TODO - set the means of the X_scaled matrix\n",
    "X_scaled_means =\n",
    "\n",
    "def test_Xscaled_y_scaled():\n",
    "    assert  X_scaled_means.tolist() == [2.1063519834785578e-16, -1.0882818581305882e-16, 0.0, -4.2127039669571156e-16,\n",
    "                                        -7.442443674957572e-16], '' \\\n",
    "    'Check there is a mismatch'\n",
    "    \n",
    "testFunc(test_Xscaled_y_scaled, 'X and y have been scaled correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepend a column of ones to X_scaled - use np.concatenate and np.ones\n",
    "X_scaled_wones = np.concatenate( ( np.ones( (len(X_scaled),1)) , X_scaled ) ,axis = 1)\n",
    "X_scaled_wones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the array of coefficients - explicity define both dimensions - set all elements to zeros\n",
    "theta = \n",
    "\n",
    "def test_theta_creation():\n",
    "    assert theta.shape == (1,6), \"We need an explicitly defined coefficients vector\"\n",
    "\n",
    "testFunc(test_theta_creation,'Theta|Array of coefficients initialized correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the cost function\n",
    "np.random.seed(100)\n",
    "X_ct = 3 * np.random.randn(5,5) + 2\n",
    "theta_ct = np.random.randn(1,5)\n",
    "y_ct = 2 * np.random.randn(5,1)\n",
    "\n",
    "\n",
    "# Create the function to compute cost - it takes X a 2 dim array - the feature matrix, y - the target values \n",
    "# a result vector of shape (m,1) where m is the number of examples and theta is the coefficient array\n",
    "# TODO\n",
    "def costFunct(X,y,theta):\n",
    "     \n",
    "\n",
    "\n",
    "def test_compute_Cost_Func():\n",
    "    assert abs(27.3752158) - costFunct(X_ct,y_ct,theta_ct ) <= 0.000001, 'Check code'\n",
    "\n",
    "testFunc( test_compute_Cost_Func,'Function to compute cost created correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in the code to compute gradient descent - \n",
    "def gradientDescent(X_n,y_n,theta_n,numIters,alpha):\n",
    "    # initialize m to the number of examples\n",
    "    m = len(X_n)\n",
    "    # jhist is the array for recording the cost after each iteration\n",
    "    # Initialize jhist to an array of (numiters,1)\n",
    "    jhist = np.zeros( (numIters,1) )\n",
    "    # for the number of iterations - we calculate each of the thetas - through matrice operations\n",
    "    #TODO - fill in the code for computing the gradient descent\n",
    "    for n in range(numIters):\n",
    "        \n",
    "        # record the history in jhistory\n",
    "        jhist[n] = costFunct(X_n,y_n,theta_n)\n",
    "    # return the theta_n and the jhist arrays\n",
    "    return theta_n, jhist\n",
    "\n",
    "numiters = 3000\n",
    "alpha = 0.001\n",
    "# Lets initialize theta once again to an array of appropriate shape of zeros\n",
    "theta = np.zeros( (1, X_scaled_wones.shape[1]) )\n",
    "#TODO - set theta, jhist using method defined aboves and using the scaled design matrix and the scaled y target\n",
    "theta, jhist = \n",
    "\n",
    "def test_theta_norm_eqn():\n",
    "    assert sum(theta_norm_eqn - np.array([ [-0.000000005],[-0.06189],[0.54565],\n",
    "                               [-0.09214],[-0.21889],[0.16194]])) < -0.000001045728, '' \\\n",
    "    'check your entries above - the values are off'\n",
    "    \n",
    "testFunc(test_theta_norm_eqn, 'Normalized equation applied correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We sill use the inverse_transform function of the Standard Scaler to get back our\n",
    "# original value applying the learned theta coefficients\n",
    "sqdiffs = sum ( ( scaler_y.inverse_transform( X_scaled_wones.dot(theta.T) ) - y.reshape(506,1) ) ** 2 )\n",
    "print sqdiffs\n",
    "#TODO - set todiffs equal to the sum of the squared deviatins from the mean\n",
    "# refer to the html notebook with you if needed\n",
    "totdiffs = \n",
    "print totdiffs\n",
    "#TODO - set the ceoff_deter to the coefficient of determination 1 less the SSE/SST (Sum of Squred Errors)/(Sum of Squared Totals)\n",
    "coeff_deter =  1 - sqdiffs/totdiffs\n",
    "print coeff_deter\n",
    "def test_coeff_determination():\n",
    "    assert abs( coeff_deter - 0.6156 ) <= 0.00001, 'Coefficient of determination by our calculations is right'\n",
    "    \n",
    "testFunc(test_coeff_determination, 'coefficient of determination computed correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets now use the normal equation\n",
    "#TODO\n",
    "# first set XTX to the dot product of X_scaled_wones transpose with X_scalced_wones\n",
    "XTX = \n",
    "#TODO\n",
    "#now set XTXInv to the inverse of XTX - use np.linalg.inv\n",
    "XTXInv = \n",
    "#TODO\n",
    "# now set XTY to the dot product of X_scaled_wones transposed and y_scaled\n",
    "XTY = \n",
    "#TODO\n",
    "# finally set theta_norm_eqn to the dot product of XTXInv and XTY\n",
    "theta_norm_eqn = \n",
    "# theta_norm_eqn = np.linalg.inv( X_scaled_wones.T.dot(X_scaled_wones) ).dot(X_scaled_wones.T.dot(y_scaled))\n",
    "def test_theta_norm_eqn():\n",
    "    assert theta_norm_eqn.tolist() == [[-5.000500225586091e-16],\n",
    " [-0.06189183622784672],\n",
    " [0.5456500454644442],\n",
    " [-0.09214132789372481],\n",
    " [-0.21889675605430906],\n",
    " [0.16194882398206223]] , '' \\\n",
    "    'check your entries above - the values are off'\n",
    "    \n",
    "testFunc(test_theta_norm_eqn, 'Normalized equation applied correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - test predictions using the coefficients obtained from the normal equation\n",
    "# TODO - compare them to the ones obtained using gradient descent and using the linear_model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# TODO - initialize lin_model to LnearRegression\n",
    "lin_model = \n",
    "#  set clf to the model obtained by fitting linear model to X_scaled, y_scaled\n",
    "clf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_coef =  clf.coef_\n",
    "# TODO - get the model score using the clf.score functoinality\n",
    "model_score =  \n",
    "def test_model_score():\n",
    "    assert abs(model_score - 0.61768 ) <= 0.00001, 'check model initiation and fitting - values are not correct'\n",
    "\n",
    "testFunc( test_model_score , 'model fitted and the coefficient of determination is right')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
