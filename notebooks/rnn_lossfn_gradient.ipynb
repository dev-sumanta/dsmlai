{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 47 characters, 21 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('rnn_input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden 100 x 21\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden 100 x 100\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output 21 x 100\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias 100 x 1\n",
    "by = np.zeros((vocab_size, 1)) # output bias 21 x 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation 21 x 1 dict with 100 elements of 21 x 1\n",
    "    xs[t][inputs[t]] = 1 # one particular column we update to 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state 100 x 21 x 21 x 1 - 100 x 1\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars - 21 x 100 x 100 x 1 - 21 x 1\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars 21 x 1\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) # 100 x 21, 100 x 100, 21 x 100\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by) # 100 x 1, 21 x 1\n",
    "  dhnext = np.zeros_like(hs[0]) # 100 x 1\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t]) # 21 x 1\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T) # 21 x 1 x 1 x 100 - 21 x 100\n",
    "    dby += dy # 21 x 1\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h  100 x 21 x 21 x 1 100 x 1\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity 100 x 1\n",
    "    dbh += dhraw # 100 x 1\n",
    "    dWxh += np.dot(dhraw, xs[t].T) # 100 x 1 x 1 x 21 - 100 x 21\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) # 100 x 1 x 1 x 100 - 100 x 100\n",
    "    dhnext = np.dot(Whh.T, dhraw) # 100 x 100 x 100 x 1 - 100 x 1\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " hennai super kings won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ips won ipsuper kings won \n",
      "----\n",
      "iter 0, loss: 76.036948\n",
      "----\n",
      " hennai super kings won ipngs won ipepspwon ipeenningsu peer iwngi skper kings won ipepsuper kings super ipngs won iper opngsrpeperi gs won ipngs won ipernnings srwon ipeper rings won ipnaipwon iper ai \n",
      "----\n",
      "iter 100, loss: 69.687700\n",
      "----\n",
      " hennai super kings won ipepsuper kings wonni kper kings won iper kings super iper kings wop ipers sepnaings seper ipnaings super aings super ipea ipkier kings won ipers won ipernningsusuper kings won  \n",
      "----\n",
      "iter 200, loss: 63.057131\n",
      "----\n",
      " hennai super kings won ipepsrwopiipepnaingsuwrpep ipuper kings won ipngs won ipers won iper kipe  ingspwonki ipeper ki psuper iper ipnaings segsuwnnnii super kings won ipers won ipernnings sr epnnings \n",
      "----\n",
      "iter 300, loss: 57.056225\n",
      "----\n",
      " hennai super kings won ipers won iper ipee  ipper kings won ipngs won iper kings super iper kings won iper kings super iper kings won ipngs won iper rper kings won ipers wonnipepnr eperinai super king \n",
      "----\n",
      "iter 400, loss: 51.625955\n",
      "----\n",
      " hennai super kings won ipers won ipngs wonni kings won iper  wonni kinpseper iper kings won ipers won iper kings won ipe super iper gs won ipngs won iperskings super iper kings won iper  iper gspergna \n",
      "----\n",
      "iter 500, loss: 46.712290\n",
      "----\n",
      " hennai super kings won iper kingsrpeper iper kings won iper kinenaikier kings won iper kineni  per kings won iper rheperkipsuper ionii super kings won iper kings super iper kings won iper  won iper ip \n",
      "----\n",
      "iter 600, loss: 42.266185\n",
      "----\n",
      " hennai super kings won iper kings super iper kings won ipeps won iper kings puper kings won ipers won iperon ipngs won iper kings won iper rper kings won ipki rper iper iper iper ipngs won iper kings  \n",
      "----\n",
      "iter 700, loss: 38.243201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-83575f836bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-e0a8ce46f117>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 100 x 1 x 1 x 21 - 100 x 21\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 100 x 1 x 1 x 100 - 100 x 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mdhnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 100 x 100 x 100 x 1 - 100 x 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clip to mitigate exploding gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
